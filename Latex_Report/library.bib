Automatically generated by Mendeley Desktop 1.13.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Bennett2008,
abstract = {The cellular mechanisms that couple activity of glutamatergic synapses with changes in blood flow, measured by a variety of techniques including the BOLD signal, have not previously been modelled. Here we provide such a model, that successfully accounts for the main observed changes in blood flow in both visual cortex and somatosensory cortex following their stimulation by high-contrast drifting grating or by single whisker stimulation, respectively. Coupling from glutamatergic synapses to smooth muscle cells of arterioles is effected by astrocytes releasing epoxyeicosatrienoic acids (EETs) onto them, following glutamate stimulation of the astrocyte. Coupling of EETs to the smooth muscle of arterioles is by means of potassium channels in their membranes, leading to hyperpolarization, relaxation and hence an increase in blood flow. This model predicts a linear increase in blood flow with increasing numbers of activated astrocytes, but a non-linear increase with increasing glutamate release. © 2007 Elsevier Ltd. All rights reserved.},
author = {Bennett, M. R. and Farnell, L. and Gibson, W. G.},
doi = {10.1016/j.jtbi.2007.08.024},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Bennett, Farnell, Gibson - 2008 - Origins of blood volume change due to glutamatergic synaptic activity at astrocytes abutting on arteri.pdf:pdf},
issn = {00225193},
journal = {Journal of Theoretical Biology},
keywords = {Astrocytes,Blood volume,Calcium waves},
number = {1},
pages = {172--185},
pmid = {17920632},
title = {{Origins of blood volume change due to glutamatergic synaptic activity at astrocytes abutting on arteriolar smooth muscle cells}},
volume = {250},
year = {2008}
}
@article{Srinivas1994,
abstract = {Traditional genetic algorithms (GAs) easily get stuck at a local optimum, and often have slow convergent speed. A novel adaptive genetic algorithm (AGA) called cloud-model-based AGA (CAGA) is proposed in this paper. Unlike conventional genetic algorithms, CAGA presents the use of cloud model to adaptively tune the probabilities of crossover p<sub>c </sub> and mutation p<sub>m</sub> depending on the fitness values of solutions. Because normal cloud models have the properties of randomness and stable tendency, CAGA is expected to realize the twin goals of maintaining diversity in the population and sustaining the convergence capacity of the GA. We compared the performance of the CAGA with that of the standard GA (SGA) and AGA in optimizing several typical functions with varying degrees of complexity and solving travelling salesman problems. In all cases studied, CAGA is greatly superior to SGA and AGA in terms of robustness and efficiency. The CAGA converges to the global optimum in far fewer generations, and gets stuck at a local optimum fewer times than SGA and AGA},
author = {Srinivas, M. and Patnaik, L. M.},
doi = {10.1109/21.286385},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Srinivas, Patnaik - 1994 - Adaptive probabilities of crossover and mutation in genetic algorithms.pdf:pdf},
isbn = {1-4244-0067-8},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {4},
pages = {656--667},
title = {{Adaptive probabilities of crossover and mutation in genetic algorithms}},
volume = {24},
year = {1994}
}
@article{Farr2011,
abstract = {Functional hyperemia is an important metabolic autoregulation mechanism by which increased neuronal activity is matched by a rapid and regional increase in blood supply. This mechanism is facilitated by a process known as "neurovascular coupling"-the orchestrated communication system involving neurons, astrocytes and arterioles. Important steps in this process are the production of EETs in the astrocyte and the release of potassium, via two potassium channels (BK and KIR), into the perivascular space. We provide a model which successfully accounts for several observations seen in experiment. The model is capable of simulating the approximate 15\% arteriolar dilation caused by a 60-s neuronal activation (modelled as a release of potassium and glutamate into the synaptic cleft). This model also successfully emulates the paradoxical experimental finding that vasoconstriction follows vasodilation when the astrocytic calcium concentration (or perivascular potassium concentration) is increased further. We suggest that the interaction of the changing smooth muscle cell membrane potential and the changing potassium-dependent resting potential of the KIR channel are responsible for this effect. Finally, we demonstrate that a well-controlled mechanism of potassium buffering is potentially important for successful neurovascular coupling. © 2011 Elsevier Ltd.},
author = {Farr, Hannah and David, Tim},
doi = {10.1016/j.jtbi.2011.07.006},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Farr, David - 2011 - Models of neurovascular coupling via potassium and EET signalling.pdf:pdf},
issn = {00225193},
journal = {Journal of Theoretical Biology},
keywords = {Cerebral blood flow regulation,Neurovascular coupling,Potassium buffering},
number = {1},
pages = {13--23},
pmid = {21781976},
publisher = {Elsevier},
title = {{Models of neurovascular coupling via potassium and EET signalling}},
url = {http://dx.doi.org/10.1016/j.jtbi.2011.07.006},
volume = {286},
year = {2011}
}
@article{Lin2003,
author = {Lin, Wen Yang and Lee, Wen Yuan and Hong, Tzung Pei},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Lin, Lee, Hong - 2003 - Adapting crossover and mutation rates in genetic algorithms.pdf:pdf},
isbn = {1016-2364},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {Crossover rate,Genetic algorithms,Mutation rate,Progressive value,Self-adaptation},
number = {5},
pages = {889--903},
title = {{Adapting crossover and mutation rates in genetic algorithms}},
volume = {19},
year = {2003}
}
@article{Diaz-Gomez2007,
abstract = {Besides the difficulty of the applicationproblem to be solved with Genetic Algorithms (GAs), an additional difficulty arises because the quality of the solution found, or the computational resources required to find it, depends on the selection of the Genetic Algorithm's characteristics. The purpose of this paper is to gain some insight into one of those characteristics: the difficult problem of finding a good initial population. We summarize previous approaches and metrics related to this problem and we suggest the center of mass as an alternative metric to measurement diversity at the population level. This theoretical approach of analysis and measure of the diversity of the initial random population is important and could be quite necessary for the design of GAs because of the relation of the initial population to other GA parameters and operators and because of its relation to the problem of premature convergence.},
author = {Diaz-Gomez, P. a. and Hougen, D. F.},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Diaz-Gomez, Hougen - 2007 - Initial Population for Genetic Algorithms A Metric Approach.pdf:pdf},
journal = {In Proceedings of the 2007 International Conference on Genetic and Evolutionary Methods (GEM'07)},
keywords = {ga,initialization,measure,problem\_difficulty},
title = {{Initial Population for Genetic Algorithms: A Metric Approach}},
year = {2007}
}
@article{Olufsen2013,
abstract = {Mathematical models have long been used for prediction of dynamics in biological systems. Recently, several efforts have been made to render these models patient specific. One way to do so is to employ techniques to estimate parameters that enable model based prediction of observed quantities. Knowledge of variation in parameters within and between groups of subjects have potential to provide insight into biological function. Often it is not possible to estimate all parameters in a given model, in particular if the model is complex and the data is sparse. However, it may be possible to estimate a subset of model parameters reducing the complexity of the problem. In this study, we compare three methods that allow identification of parameter subsets that can be estimated given a model and a set of data. These methods will be used to estimate patient specific parameters in a model predicting baroreceptor feedback regulation of heart rate during head-up tilt. The three methods include: structured analysis of the correlation matrix, analysis via singular value decomposition followed by QR factorization, and identification of the subspace closest to the one spanned by eigenvectors of the model Hessian. Results showed that all three methods facilitate identification of a parameter subset. The "best" subset was obtained using the structured correlation method, though this method was also the most computationally intensive. Subsets obtained using the other two methods were easier to compute, but analysis revealed that the final subsets contained correlated parameters. In conclusion, to avoid lengthy computations, these three methods may be combined for efficient identification of parameter subsets.},
author = {Olufsen, Mette S. and Ottesen, Johnny T.},
doi = {10.1007/s00285-012-0535-8},
file = {:P$\backslash$:/temp/Internship/Project/Report/Report\_GA/Papers/Olufsen, Ottesen - 2013 - A practical approach to parameter estimation applied to model predicting heart rate regulation.pdf:pdf},
isbn = {0303-6812},
issn = {03036812},
journal = {Journal of Mathematical Biology},
keywords = {Inverse problems,Medical applications,Nonlinear heart rate model,Parameter estimation,Patient specific modeling,Simulation and modeling,Subset selection},
number = {1},
pages = {39--68},
pmid = {22588357},
title = {{A practical approach to parameter estimation applied to model predicting heart rate regulation}},
volume = {67},
year = {2013}
}
@article{Melanie1996,
abstract = {Science arises from the very human desire to understand and control the world. Over the course of history, we humans have gradually built up a grand edifice of knowledge that enables us to predict, to varying extents, the weather, the motions of the planets, solar and lunar eclipses, the courses of diseases, the rise and fall of economic growth, the stages of language development in children, and a vast panorama of other natural, social, and cultural phenomena. More recently we have even come to understand some fundamental limits to our abilities to predict. Over the eons we have developed increasingly complex means to control many aspects of our lives and our interactions with nature, and we have learned, often the hard way, the extent to which other aspects are uncontrollable. The advent of electronic computers has arguably been the most revolutionary development in the history of science and technology. This ongoing revolution is profoundly increasing our ability to predict and control nature in ways that were barely conceived of even half a century ago. For many, the crowning achievements of this revolution will be the creation—in the form of computer programs—of new species of intelligent beings, and even of new forms of life. The goals of creating artificial intelligence and artificial life can be traced back to the very beginnings of the computer age. The earliest computer scientists—Alan Turing, John von Neumann, Norbert Wiener, and others—were motivated in large part by visions of imbuing computer programs with intelligence, with the life−like ability to self−replicate, and with the adaptive capability to learn and to control their environments. These early pioneers of computer science were as much interested in biology and psychology as in electronics, and they looked to natural systems as guiding metaphors for how to achieve their visions. It should be no surprise, then, that from the earliest days computers were applied not only to calculating missile trajectories and deciphering military codes but also to modeling the brain, mimicking human learning, and simulating biological evolution. These biologically motivated computing activities have waxed and waned over the years, but since the early 1980s they have all undergone a resurgence in the computation research community. The first has grown into the field of neural networks, the second into machine learning, and the third into what is now called "evolutionary computation," of which genetic algorithms are the most prominent example.},
author = {Melanie, Mitchell},
doi = {10.1016/S0898-1221(96)90227-8},
file = {:C$\backslash$:/Users/mwb76/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melanie - 1996 - An introduction to genetic algorithms.pdf:pdf},
isbn = {0-262-13316-4},
issn = {08981221},
journal = {Cambridge, Massachusetts London, England, \ldots},
pages = {162},
title = {{An introduction to genetic algorithms}},
year = {1996}
}
